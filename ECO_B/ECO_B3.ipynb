{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "import multiprocessing as mp\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 161\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "XTRA_FEAT   = 1 #masscart, masspole, length\n",
    "N_ACTIONS   = env.action_space.n\n",
    "N_STATES    = env.observation_space.shape[0]  +  XTRA_FEAT \n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  STI6WR1Z_13_48_42\n",
      "NN-MODEL FILENAME:  ./models/STI6WR1Z_13_48_42_NN.pt\n"
     ]
    }
   ],
   "source": [
    "RNDM_STRING = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)) + datetime.now().strftime(\"_%H_%M_%S\")\n",
    "print(\"ID: \",RNDM_STRING)\n",
    "MODEL_FILENAME = './models/'+ RNDM_STRING + \"_NN\" + \".pt\"\n",
    "print(\"NN-MODEL FILENAME: \", MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndim_grid(start,stop, granularity):\n",
    "    # Set number of dimensions\n",
    "    ndims = len(start)\n",
    "\n",
    "    # List of ranges across all dimensions\n",
    "    L = [np.linspace(start[i],stop[i],granularity[i]) for i in range(ndims)]\n",
    "\n",
    "    # Finally use meshgrid to form all combinations corresponding to all \n",
    "    # dimensions and stack them as M x ndims array\n",
    "    return np.hstack((np.meshgrid(*L))).swapaxes(0,1).reshape(ndims,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(value, borders):\n",
    "    c_pos_val, c_vel_val, p_ang_val, p_vel_val, length_val   = value\n",
    "    c_pos_s  , c_vel_s  ,p_ang_s   , p_vel_s  , length_s     = borders\n",
    "    \n",
    "    indx = np.empty_like(value).astype(np.uint)\n",
    "    \n",
    "    for i in range(value.shape[0]):\n",
    "        if value[i] > borders[i].max():\n",
    "            indx[i] = borders[i].argmax()\n",
    "        else:\n",
    "            indx[i] = np.where(borders[i] >= value[i])[0][0].astype(np.uint)\n",
    "    return indx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NODES:  10\n",
      "Number of EPISODES per NODE 10\n"
     ]
    }
   ],
   "source": [
    "T_LR           = 1e-2\n",
    "T_GAMMA        = 0.99\n",
    "T_EPSILON      = 0.98\n",
    "\n",
    "NO_OF_NODES    = 10\n",
    "NO_OF_EPISODES = 100\n",
    "TIMESTEP_LIMIT = 200\n",
    "\n",
    "print(\"Number of NODES: \", NO_OF_NODES)\n",
    "print(\"Number of EPISODES per NODE\", NO_OF_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "HIDDEN_LAYER        = 50\n",
    "BATCH_SIZE          = 32\n",
    "NN_LR               = 1e-3  # learning rate\n",
    "NN_GAMMA            = 0.9   # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "TERMINAL_BIAS       = 0.5   # no. of terminal memories in batch\n",
    "MIN_MEMORY_CAP      = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ITERATIONS:  3\n"
     ]
    }
   ],
   "source": [
    "MAX_NO_OF_ITERATIONS = 10\n",
    "MAX_NN_ITERATIONS    = 7000\n",
    "print(\"Number of ITERATIONS: \",MAX_NO_OF_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self, ):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.fc1 = nn.Linear(N_STATES, 50)\n",
    "#         nn.init.kaiming_uniform_(self.fc1.weight)   # initialization\n",
    "#         self.out = nn.Linear(50, N_ACTIONS)\n",
    "#         nn.init.xavier_uniform_(self.out.weight)   # initialization\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         actions_value = self.out(x)\n",
    "#         return actions_value\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.adv = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.adv.weight) \n",
    "    \n",
    "        self.val = nn.Linear(HIDDEN_LAYER, 1)\n",
    "        nn.init.xavier_uniform_(self.val.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        adv = self.adv(x)\n",
    "        val = self.val(x)\n",
    "        \n",
    "        return val + adv - adv.mean()\n",
    "    \n",
    "class D3QN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "        \n",
    "        self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "        self.good_memory         = np.zeros((MIN_MEMORY_CAP ,N_STATES*2+2))#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "        self.bad_memory          = np.zeros((MIN_MEMORY_CAP , N_STATES*2+2))#np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=NN_LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def get_qvals(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        actions_value = actions_value.data.numpy().astype(np.float16)\n",
    "        return actions_value\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        good_sample_index_limit = min(MIN_MEMORY_CAP, self.good_memory_counter)\n",
    "        bad_sample_index_limit = min(MIN_MEMORY_CAP, self.bad_memory_counter)\n",
    "\n",
    "        good_sample_index = np.random.choice(int(good_sample_index_limit), int(BATCH_SIZE-int(BATCH_SIZE*TERMINAL_BIAS)))\n",
    "        bad_sample_index  = np.random.choice(int(bad_sample_index_limit),  int(BATCH_SIZE*TERMINAL_BIAS))\n",
    "\n",
    "        b_good_memory = self.good_memory[good_sample_index, :]\n",
    "        b_bad_memory  = self.bad_memory[bad_sample_index, :]\n",
    "        b_memory      = np.vstack((b_good_memory,b_bad_memory))\n",
    "        \n",
    "        b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(BATCH_SIZE, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + NN_GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "        loss     = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp(MIN_VAL, VAL, MAX_VAL):\n",
    "    return max(MIN_VAL, min(VAL, MAX_VAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSOLUTE LIMITS ON STATE VALUES\n",
    "C_POS_ABS_MAX =  2.4\n",
    "C_POS_ABS_MIN = -2.4\n",
    "\n",
    "C_VEL_ABS_MAX =  5\n",
    "C_VEL_ABS_MIN = -5\n",
    "\n",
    "P_ANG_ABS_MAX =  0.25\n",
    "P_ANG_ABS_MIN = -0.25\n",
    "\n",
    "P_VEL_ABS_MAX =  6\n",
    "P_VEL_ABS_MIN = -6\n",
    "\n",
    "LENGTH_ABS_MAX = 0.925\n",
    "LENGTH_ABS_MIN = 0.375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI_GRAIN =  10\n",
      "LO_GRAIN =  10\n"
     ]
    }
   ],
   "source": [
    "# SET GRANULARITY\n",
    "HI_GRAIN = 30\n",
    "LO_GRAIN = 30\n",
    "print(\"HI_GRAIN = \", HI_GRAIN)\n",
    "print(\"LO_GRAIN = \", LO_GRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_node_run(node_id, boundary, iteration):\n",
    "\n",
    "    # SET SEED\n",
    "    ###############################################\n",
    "    my_seed = seed + node_id + iteration\n",
    "    random.seed(my_seed)\n",
    "    torch.manual_seed(my_seed)\n",
    "    np.random.seed(my_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(my_seed)\n",
    "    my_env = env\n",
    "    my_env.seed(my_seed)\n",
    "    ###############################################\n",
    "    \n",
    "    # Mean values of pole length deviate by 40% from original value\n",
    "    scaling_factor = 0.8 + (1.8 - 0.8) * (node_id + 1)/(NO_OF_NODES)\n",
    "    ORIGNAL_LENGTH = 0.5    \n",
    "    length_mean    = ORIGNAL_LENGTH * scaling_factor\n",
    "\n",
    "    # SET STATE VALUE BORDERS\n",
    "    ###############################################\n",
    "    [C_POS_MAX, C_VEL_MAX, P_ANG_MAX, P_VEL_MAX, LENGTH_MAX,\n",
    "     C_POS_MIN, C_VEL_MIN, P_ANG_MIN, P_VEL_MIN, LENGTH_MIN]  = [C_POS_ABS_MAX, C_VEL_ABS_MAX, P_ANG_ABS_MAX, P_VEL_ABS_MAX, LENGTH_ABS_MAX,\n",
    "                                                                 C_POS_ABS_MIN, C_VEL_ABS_MIN, P_ANG_ABS_MIN, P_VEL_ABS_MIN, LENGTH_ABS_MIN]\n",
    "    ###############################################\n",
    "#     LENGTH_MAX = length_mean + 0.05\n",
    "#     LENGTH_MIN = length_mean - 0.05\n",
    "    # CREATE STATE TABLE BORDERS\n",
    "    ###############################################\n",
    "    c_pos_s  = np.linspace(C_POS_MIN,  C_POS_MAX,  HI_GRAIN)\n",
    "    c_vel_s  = np.linspace(C_VEL_MIN,  C_VEL_MAX,  HI_GRAIN)\n",
    "    p_ang_s  = np.linspace(P_ANG_MIN,  P_ANG_MAX,  HI_GRAIN)\n",
    "    p_vel_s  = np.linspace(P_VEL_MIN,  P_VEL_MAX,  HI_GRAIN)\n",
    "    length_s = np.linspace(LENGTH_MIN, LENGTH_MAX, LO_GRAIN)\n",
    "\n",
    "    borders = [c_pos_s, c_vel_s, p_ang_s, p_vel_s, length_s]\n",
    "    ###############################################\n",
    "    \n",
    "    state_combinations = ndim_grid([C_POS_MIN, C_VEL_MIN, P_ANG_MIN, P_VEL_MIN, LENGTH_MIN ],\n",
    "                                    [C_POS_MAX, C_VEL_MAX, P_ANG_MAX, P_VEL_MAX, LENGTH_MAX ],\n",
    "                                    [HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN   ])\n",
    "    \n",
    "#     my_dqn = D3QN()\n",
    "#     my_dqn.eval_net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "#     my_dqn.eval_net.eval()\n",
    "#     my_QFILE   = './Q_NPY/' + RNDM_STRING + str(node_id) + 'QFILE' + \".npy\"\n",
    "    my_Q_TABLE = np.load(node_QFILE)\n",
    "    my_Q_TABLE = my_Q_TABLE[:,:,:,:, int(node_id*LO_GRAIN):int((node_id+1)*LO_GRAIN)]\n",
    "#     my_Q_TABLE = my_dqn.get_qvals(state_combinations).reshape(HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN , -1)\n",
    "\n",
    "    time_rec                = np.zeros(NO_OF_EPISODES)\n",
    "    level_up_flag           = False\n",
    "    PERFECT_RUN_COUNTER     = 10\n",
    "    PERFECT_RUNS_HIGH_SCORE = 10\n",
    "    level_up_metric         = 195\n",
    "\n",
    "    exp_rec      = np.empty(N_STATES * 2 + 2)\n",
    "    \n",
    "    if iteration < 3:\n",
    "        my_EPSILON   = (iteration+1) * 0.2 + np.random.uniform(-0.1,0.1)\n",
    "    else:\n",
    "        my_EPSILON   = T_EPSILON + np.random.uniform(-0.01,0.01)\n",
    "        \n",
    "    my_LR        = T_LR\n",
    "    \n",
    "    while True:\n",
    "        i_episode = 0\n",
    "        \n",
    "        while i_episode < NO_OF_EPISODES:\n",
    "            ep_exp_rec = np.empty(N_STATES * 2 + 2)\n",
    "            time_steps = 0\n",
    "\n",
    "                     \n",
    "            my_env.length   = length_mean + np.random.uniform(-0.025,0.025)\n",
    "            xtra = [my_env.length]\n",
    "            \n",
    "            s = my_env.reset()\n",
    "            s = np.append(s, xtra)\n",
    "            \n",
    "            while True:\n",
    "                time_steps += 1\n",
    "                \n",
    "                #get state\n",
    "                this_state = tuple(discretize(s, borders))\n",
    "                \n",
    "                # choose action\n",
    "                if np.random.uniform() > my_EPSILON:   # greedy\n",
    "                    a = np.random.randint(0, N_ACTIONS)\n",
    "                else:\n",
    "                    a = my_Q_TABLE[this_state][:].argmax()\n",
    "\n",
    "                 # take action\n",
    "                s_, r, done, info = my_env.step(a)\n",
    "                s_ = np.append(s_, xtra)\n",
    "\n",
    "                if done:\n",
    "                    r = -1\n",
    "                    if time_steps >= TIMESTEP_LIMIT:\n",
    "                        r = 1\n",
    "                \n",
    "                #store experience\n",
    "                experience = np.hstack((s,a,r,s_))\n",
    "                exp_rec = np.vstack((exp_rec, experience))\n",
    "\n",
    "                #discretize next_state\n",
    "                next_state = tuple(discretize(s_, borders))\n",
    "\n",
    "                # learn\n",
    "#                 my_Q_TABLE[this_state][a] = my_Q_TABLE[this_state][a] + my_LR * (r + T_GAMMA * my_Q_TABLE[next_state].max() - my_Q_TABLE[this_state][a])\n",
    "                \n",
    "                if done or time_steps >= TIMESTEP_LIMIT:\n",
    "                    time_rec[i_episode] = time_steps\n",
    "                    break\n",
    "                s = s_\n",
    "\n",
    "            i_episode += 1\n",
    "        if i_episode >= NO_OF_EPISODES:\n",
    "            i_episode = 0\n",
    "            break\n",
    "\n",
    "    exp_rec = np.delete(exp_rec, 0, 0)\n",
    "#         message = \"NODE#\"+str(node_id) +\" MAIN Q:\"+ str(new_Q_TABLE.mean()) +\"\\t\" + \"NODE Q:\" + str(my_Q_TABLE.mean())\n",
    "#         print(message)\n",
    "    \n",
    "    # GET NEW STATE BOUNDARIES: +/- 25% of experienced state\n",
    "    [C_POS_MAX, C_VEL_MAX, P_ANG_MAX, P_VEL_MAX, LENGTH_MAX ] = [exp_rec[:,i].max() for i in range(N_STATES)]\n",
    "    [C_POS_MIN, C_VEL_MIN, P_ANG_MIN, P_VEL_MIN, LENGTH_MIN ] = [exp_rec[:,i].min() for i in range(N_STATES)]\n",
    "    \n",
    "#     LENGTH_MAX = exp_rec[:,4].max() + 0.25*np.abs(exp_rec[:,4].max())\n",
    "#     LENGTH_MIN = exp_rec[:,4].min() - 0.25*np.abs(exp_rec[:,4].min())\n",
    "\n",
    "    # CLAMP STATE VALUE BORDERS\n",
    "    ###############################################\n",
    "#     C_POS_MAX = clamp(C_POS_ABS_MIN, C_POS_MAX, C_POS_ABS_MAX)\n",
    "#     C_POS_MIN = clamp(C_POS_ABS_MIN, C_POS_MIN, C_POS_ABS_MAX) \n",
    "\n",
    "#     C_VEL_MAX = clamp(C_VEL_ABS_MIN, C_VEL_MAX, C_VEL_ABS_MAX)\n",
    "#     C_VEL_MIN = clamp(C_VEL_ABS_MIN, C_VEL_MIN, C_VEL_ABS_MAX) \n",
    "\n",
    "#     P_ANG_MAX = clamp(P_ANG_ABS_MIN, P_ANG_MAX, P_ANG_ABS_MAX)\n",
    "#     P_ANG_MIN = clamp(P_ANG_ABS_MIN, P_ANG_MIN, P_ANG_ABS_MAX) \n",
    "\n",
    "#     P_VEL_MAX = clamp(P_VEL_ABS_MIN, P_VEL_MAX, P_VEL_ABS_MAX)\n",
    "#     P_VEL_MIN = clamp(P_VEL_ABS_MIN, P_VEL_MIN, P_VEL_ABS_MAX) \n",
    "\n",
    "    LENGTH_MAX = clamp(LENGTH_ABS_MIN, LENGTH_MAX + 0.075, LENGTH_ABS_MAX)\n",
    "    LENGTH_MIN = clamp(LENGTH_ABS_MIN, LENGTH_MIN - 0.075, LENGTH_ABS_MAX)\n",
    "    ###############################################\n",
    "    \n",
    "    newboundary = [C_POS_MAX, C_VEL_MAX, P_ANG_MAX, P_VEL_MAX, LENGTH_MAX,\n",
    "                   C_POS_MIN, C_VEL_MIN, P_ANG_MIN, P_VEL_MIN, LENGTH_MIN]\n",
    "    \n",
    "    return exp_rec, time_rec, newboundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_serial_timesteps   = 0\n",
    "total_parallel_timesteps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 618.56 MiB, increment: 429.30 MiB\n",
      "Quantization TIME:  0.01 minutes\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "# my_dqn = D3QN()\n",
    "dqn = D3QN()\n",
    "\n",
    "# INITIALIZE BOUNDARY TO MAXIMUM VALUES\n",
    "init_node_boundary = [C_POS_ABS_MAX, C_VEL_ABS_MAX, P_ANG_ABS_MAX, P_VEL_ABS_MAX, LENGTH_ABS_MAX,\n",
    "                      C_POS_ABS_MIN, C_VEL_ABS_MIN, P_ANG_ABS_MIN, P_VEL_ABS_MIN, LENGTH_ABS_MIN]\n",
    "\n",
    "\n",
    "\n",
    "# CREATE STATE COMBINATIONS\n",
    "# ###############################################\n",
    "\n",
    "init_state_combinations = ndim_grid([C_POS_ABS_MIN, C_VEL_ABS_MIN, P_ANG_ABS_MIN, P_VEL_ABS_MIN, LENGTH_ABS_MIN ],\n",
    "                                    [C_POS_ABS_MAX, C_VEL_ABS_MAX, P_ANG_ABS_MAX, P_VEL_ABS_MAX, LENGTH_ABS_MAX ],\n",
    "                                    [HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN*NO_OF_NODES   ])\n",
    "\n",
    "\n",
    "\n",
    "# GET Q-VALUES \n",
    "start = timeit.default_timer()\n",
    "%memit init_q_table = dqn.get_qvals(init_state_combinations).reshape(HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN*NO_OF_NODES , -1)\n",
    "stop = timeit.default_timer()\n",
    "print(\"Quantization TIME: \", np.round((stop-start)/60,2), \"minutes\")\n",
    "    \n",
    "# SAVE QFILE\n",
    "node_QFILE = './Q_NPY/' + RNDM_STRING + 'QFILE' + \".npy\"\n",
    "np.save(node_QFILE, init_q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-11:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bhootmali/anaconda3/envs/eno/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT FOR CHECKING NN MODEL\n",
    "v_env = gym.make('CartPole-v0')\n",
    "v_env.seed(seed*2)\n",
    "\n",
    "# CREATE A POOL OF PROCESSES\n",
    "pool = mp.Pool(NO_OF_NODES)\n",
    "\n",
    "# SET INITIAL NODE BOUDNARIES FOR ALL NODES\n",
    "node_boundaries = [init_node_boundary] * NO_OF_NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ITERATION # 0\n",
      "MEAN TABULAR EPSILON =  0.2\n",
      "TABULAR LR      =  0.01\n",
      "SMALLEST TIMESTEP in ITERATION 0: 8\n",
      "REAL TIME TO GENERATE 1889 EXPERIENCES:0:00:00.152177\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD MEMORY COUNTER:  1789\n",
      "BAD MEMORY COUNTER:  100\n",
      "Training Neural Network for 7000 iterations @ LR =  0.0001\n",
      "16 TERMINAL EXPERIENCES IN A BATCH SIZE OF 32\n",
      "Validating... MEAN TIME:  95.945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f193f2d87c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mnn_level_up_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_ITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;31m#validate by running for TIMESTEP_LIMIT iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_iter\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_ITERATIONS\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNN_ITERATIONS\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3f3cdc6aa5c8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/eno/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while iteration < MAX_NO_OF_ITERATIONS:\n",
    "    if iteration < 3:\n",
    "        node_EPSILON   = (iteration+1) * 0.2\n",
    "    else:\n",
    "        node_EPSILON   = T_EPSILON\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"ITERATION #\", iteration)\n",
    "    print(\"MEAN TABULAR EPSILON = \", node_EPSILON)\n",
    "    print(\"TABULAR LR      = \", T_LR)\n",
    "\n",
    "    tic = datetime.now()\n",
    "    \n",
    "    # MAP GYM ENVIRONMENT TO EACH PROCESS IN THE POOL\n",
    "    ##################################################################\n",
    "    iter_list = [iteration] * NO_OF_NODES\n",
    "    arg_list = [arg for arg in zip(range(NO_OF_NODES), node_boundaries, iter_list)]\n",
    "    result   = pool.starmap(mp_node_run, arg_list)\n",
    "    ##################################################################\n",
    "    \n",
    "    # GATHER RESULTS\n",
    "    ##################################################################\n",
    "    node_boundaries = np.array([item[2] for item in result])\n",
    "    node_time_rec   = np.array([item[1] for item in result])\n",
    "    node_exp        = np.array([item[0] for item in result])\n",
    "    \n",
    "    all_exp         = np.array([item for each_node_exp in node_exp \n",
    "                                    for episode_exp in each_node_exp \n",
    "                                    for item in episode_exp]).reshape(-1,N_STATES*2+2)\n",
    "    total_parallel_timesteps += node_time_rec.max()\n",
    "    total_serial_timesteps   += node_time_rec.sum()\n",
    "    EXP_GEN = node_time_rec.sum().astype(int)\n",
    "\n",
    "    print(\"SMALLEST TIMESTEP in ITERATION {:d}: {:d}\".format(iteration, node_time_rec.min().astype(int)))\n",
    "    print(\"REAL TIME TO GENERATE {:d} EXPERIENCES:{}\".format(EXP_GEN, (datetime.now()-tic)))\n",
    "    ##################################################################\n",
    "\n",
    "    # PLOT EXPERIENCES\n",
    "    ##################################################################\n",
    "    node_avg_time = node_time_rec.mean(axis=1)\n",
    "    node_std_time = node_time_rec.std(axis=1)\n",
    "    node_max_time = node_time_rec.max(axis=1)\n",
    "    node_min_time = node_time_rec.min(axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize = (15,3))\n",
    "    ax2 = fig.add_subplot(1, 1, 1)\n",
    "    ax2.set_title(\"Q-table Performance\")\n",
    "    ax2.bar(range(NO_OF_NODES) , node_max_time, alpha = 0.1, color = 'r', edgecolor = 'black', capsize=7 )\n",
    "    ax2.bar(range(NO_OF_NODES) , node_avg_time, alpha = 0.5, color = 'g', edgecolor = 'black', capsize=7 )\n",
    "    ax2.bar(range(NO_OF_NODES) , node_min_time, alpha = 0.4, color = 'r', edgecolor = 'black', capsize=7 )\n",
    "\n",
    "    ax2.plot(np.ones_like(node_avg_time)*200, 'g--')\n",
    "    ax2.set_ylabel('Mean Node Lifetime',color = 'g')\n",
    "    ax2.set_ylim(0,TIMESTEP_LIMIT+10)\n",
    "    fig.tight_layout()\n",
    "    ax2.grid()\n",
    "    plt.show()\n",
    "    ##################################################################\n",
    "    \n",
    "    if node_min_time.min() > 195:\n",
    "        final_result = \"SUCCESS\"\n",
    "        break\n",
    "\n",
    "    # SEGREGATE AND STORE EXPERIENCES\n",
    "    ##################################################################\n",
    "    good_mem = all_exp[all_exp[:,N_STATES+1] == 1]    \n",
    "    bad_mem  = all_exp[all_exp[:,N_STATES+1] < 1]\n",
    "\n",
    "\n",
    "    dqn.good_memory = np.insert(dqn.good_memory, 0, good_mem , 0)\n",
    "    dqn.good_memory_counter += good_mem.shape[0]\n",
    "\n",
    "    dqn.bad_memory  = np.insert(dqn.bad_memory, 0, bad_mem , 0)\n",
    "    dqn.bad_memory_counter += bad_mem.shape[0]\n",
    "\n",
    "    dqn.good_memory = dqn.good_memory[:MIN_MEMORY_CAP,:]\n",
    "    dqn.bad_memory = dqn.bad_memory[:MIN_MEMORY_CAP,:]\n",
    "\n",
    "    NN_ITERATIONS = MAX_NN_ITERATIONS\n",
    "\n",
    "    print(\"GOOD MEMORY COUNTER: \", min(MIN_MEMORY_CAP, dqn.good_memory_counter))\n",
    "    print(\"BAD MEMORY COUNTER: \", min(MIN_MEMORY_CAP, dqn.bad_memory_counter))\n",
    "    ##################################################################\n",
    "\n",
    "    # LEARN\n",
    "    ##################################################################\n",
    "    if iteration < 3:\n",
    "        NN_LR = 1e-4\n",
    "    else:\n",
    "        NN_LR = 1e-3\n",
    "    print(\"Training Neural Network for\", NN_ITERATIONS, \"iterations\", \"@ LR = \", NN_LR)\n",
    "    print(int(BATCH_SIZE*TERMINAL_BIAS),\"TERMINAL EXPERIENCES IN A BATCH SIZE OF\",BATCH_SIZE)\n",
    "    tic=datetime.now()\n",
    "    nn_level_up_metric = 0\n",
    "    for nn_iter in range(NN_ITERATIONS):\n",
    "        dqn.learn()\n",
    "        #validate by running for TIMESTEP_LIMIT iterations\n",
    "        if(nn_iter%int(NN_ITERATIONS/5) == int(NN_ITERATIONS/5)-1):\n",
    "            print(\"Validating... \",end=\"\")\n",
    "            time_rec = []\n",
    "            v_env.length   = np.random.uniform(LENGTH_ABS_MIN, LENGTH_ABS_MAX)\n",
    "            v_xtra = [v_env.length]\n",
    "            for i_episode in range(TIMESTEP_LIMIT):\n",
    "                time_step = 0\n",
    "                s = v_env.reset()\n",
    "                s = np.append(s, v_xtra)\n",
    "\n",
    "                while True:\n",
    "                    time_step += 1 \n",
    "                    a = dqn.choose_greedy_action(s)\n",
    "                    s_, r, done, info = v_env.step(a)\n",
    "                    s_ = np.append(s_, v_xtra)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "                    s = s_\n",
    "                time_rec = np.append(time_rec, time_step)\n",
    "            mean_time = time_rec.mean()\n",
    "            print(\"MEAN TIME: \", mean_time)\n",
    "            if mean_time >= nn_level_up_metric:\n",
    "                nn_level_up_metric = mean_time\n",
    "                torch.save(dqn.eval_net.state_dict(), MODEL_FILENAME)\n",
    "\n",
    "    print(\"TRAINING TIME:{}\".format(datetime.now()-tic))\n",
    "    ##################################################################\n",
    "\n",
    "    # CHECK PERFORMANCE OF THE BEST MODEL\n",
    "    ##################################################################\n",
    "    best_dqn = D3QN()\n",
    "    best_dqn.eval_net.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    best_dqn.eval_net.eval()\n",
    "\n",
    "    time_rec = []\n",
    "    for i_episode in range(TIMESTEP_LIMIT):\n",
    "        env.length   = np.random.uniform(LENGTH_ABS_MIN, LENGTH_ABS_MAX)\n",
    "        Xtra = [env.length]\n",
    "        time_step = 0\n",
    "        s = env.reset()\n",
    "        s = np.append(s, Xtra)\n",
    "\n",
    "        while True:\n",
    "    #         env.render()\n",
    "            time_step += 1 \n",
    "            a = best_dqn.choose_greedy_action(s)\n",
    "            s_, r, done, info = env.step(a)\n",
    "            s_ = np.append(s_, Xtra)\n",
    "            if done:\n",
    "                break\n",
    "            s = s_\n",
    "        time_rec = np.append(time_rec, time_step)\n",
    "\n",
    "    fig = plt.figure(figsize = (15,3))\n",
    "    ax2 = fig.add_subplot(1, 1, 1)\n",
    "    data = time_rec\n",
    "    ax2.plot(data, color = 'm')\n",
    "    ax2.plot(np.ones_like(data)*200, 'm--')\n",
    "    ax2.set_title('Neural Network Performance using BEST MODEL ')\n",
    "    ax2.set_ylabel('Time Steps',color = 'm')\n",
    "    ax2.set_ylim(0,TIMESTEP_LIMIT+10)\n",
    "    fig.tight_layout()\n",
    "    ax2.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "        \n",
    "    ##################################################################\n",
    "    \n",
    "    # CREATE ONE LARGE Q-TABLES FROM WHICH THE NODES STRIP\n",
    "    ##################################################################\n",
    "        \n",
    "    node_state_combinations = ndim_grid([C_POS_ABS_MIN, C_VEL_ABS_MIN, P_ANG_ABS_MIN, P_VEL_ABS_MIN, LENGTH_ABS_MIN ],\n",
    "                                        [C_POS_ABS_MAX, C_VEL_ABS_MAX, P_ANG_ABS_MAX, P_VEL_ABS_MAX, LENGTH_ABS_MAX ],\n",
    "                                        [HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN * NO_OF_NODES  ])\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    # GET Q-VALUES \n",
    "    %memit node_q_table = best_dqn.get_qvals(node_state_combinations).reshape(HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN*NO_OF_NODES , -1).astype(np.float16)\n",
    "\n",
    "    # SAVE QFILE\n",
    "    np.save(node_QFILE, node_q_table)\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "#     ##################################################################\n",
    "    \n",
    "#     # CREATE INDIVIDUALIZED Q-TABLES FOR THE NODES\n",
    "#     ##################################################################\n",
    "    \n",
    "#     for node_id in range(NO_OF_NODES):\n",
    "# #         # SET STATE VALUE BORDERS AS REQUESTED BY THE NODE\n",
    "# #         ###############################################\n",
    "#         [C_POS_MAX, C_VEL_MAX, P_ANG_MAX, P_VEL_MAX, LENGTH_MAX,\n",
    "#          C_POS_MIN, C_VEL_MIN, P_ANG_MIN, P_VEL_MIN, LENGTH_MIN]  = node_boundaries[node_id]\n",
    "# #         ###############################################\n",
    "#         print(node_id,'-max-',node_boundaries[node_id][:N_STATES])\n",
    "#         print(node_id,'-min-',node_boundaries[node_id][N_STATES:])\n",
    "#         print(\"\")\n",
    "        \n",
    "# #         node_boundaries[node_id] = [C_POS_ABS_MAX, C_VEL_ABS_MAX, P_ANG_ABS_MAX, P_VEL_ABS_MAX, LENGTH_MAX,\n",
    "# #                                     C_POS_ABS_MIN, C_VEL_ABS_MIN, P_ANG_ABS_MIN, P_VEL_ABS_MIN, LENGTH_MIN]\n",
    "# #         # CREATE STATE COMBINATIONS\n",
    "# #         ###############################################\n",
    "\n",
    "#     node_state_combinations = ndim_grid([C_POS_ABS_MIN, C_VEL_ABS_MIN, P_ANG_ABS_MIN, P_VEL_ABS_MIN, LENGTH_ABS_MIN ],\n",
    "#                                         [C_POS_ABS_MAX, C_VEL_ABS_MAX, P_ANG_ABS_MAX, P_VEL_ABS_MAX, LENGTH_ABS_MAX ],\n",
    "#                                         [HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN   ])\n",
    "#         ###############################################\n",
    "#     start = timeit.default_timer()\n",
    "#     # GET Q-VALUES \n",
    "#     %memit node_q_table = best_dqn.get_qvals(node_state_combinations).reshape(HI_GRAIN , HI_GRAIN , HI_GRAIN , HI_GRAIN , LO_GRAIN , -1).astype(np.float16)\n",
    "\n",
    "#     # SAVE QFILE\n",
    "#     np.save(node_QFILE, node_q_table)\n",
    "#         #############################################################################################################################################\n",
    "#     stop = timeit.default_timer()\n",
    "#     print(\"Quantization TIME: \", np.round((stop-start)/60,2), \"minutes\")\n",
    "    iteration += 1\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Parallel Timesteps : \", total_parallel_timesteps)\n",
    "print(\"Total Serial Timesteps   : \", total_serial_timesteps)\n",
    "print(\"Speed-up                 :  {:6.2f}\".format(total_serial_timesteps/total_parallel_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()\n",
    "pool.join()\n",
    "if iteration == MAX_NO_OF_ITERATIONS:\n",
    "    final_result = \"FAILURE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:6d} {} {:3d} {:3d} {:7d} {:10d} {:10.2f}\".format(seed, final_result, int(node_min_time.min()), int(iteration), int(total_parallel_timesteps), int(total_serial_timesteps), total_serial_timesteps/total_parallel_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
