{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9295\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "HIDDEN_LAYER        = 50\n",
    "BATCH_SIZE          = 32\n",
    "LR                  = 1e-4  # learning rate\n",
    "EPSILON             = 0.9   # greedy policy\n",
    "GAMMA               = 0.9   # reward discount\n",
    "TARGET_REPLACE_ITER = 500   # target update frequency\n",
    "MEMORY_CAPACITY     = 100000\n",
    "TERMINAL_BIAS       = 0.5   # no. of terminal memories in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTRA_FEAT   = 0 #masscart, masspole, length\n",
    "N_ACTIONS   = env.action_space.n \n",
    "N_STATES    = env.observation_space.shape[0] + XTRA_FEAT\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou: \n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "        \n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6] \n",
    "            \n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            else: # downward search, always search for a higher priority node\n",
    "                \n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                    \n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "            \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "    \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "        \n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "            \n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "                                   \n",
    "            b_idx[i]= index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "        \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SumTree\n",
    "# # a binary tree data structure where the parent’s value is the sum of its children\n",
    "# class SumTree:\n",
    "#     write = 0\n",
    "\n",
    "#     def __init__(self, capacity):\n",
    "#         self.capacity = capacity\n",
    "#         self.tree = np.zeros(2 * capacity - 1)\n",
    "#         self.data = np.zeros(capacity, dtype=object)\n",
    "#         self.n_entries = 0\n",
    "\n",
    "#     # update to the root node\n",
    "#     def _propagate(self, idx, change):\n",
    "#         parent = (idx - 1) // 2\n",
    "\n",
    "#         self.tree[parent] += change\n",
    "\n",
    "#         if parent != 0:\n",
    "#             self._propagate(parent, change)\n",
    "\n",
    "#     # find sample on leaf node\n",
    "#     def _retrieve(self, idx, s):\n",
    "#         left = 2 * idx + 1\n",
    "#         right = left + 1\n",
    "\n",
    "#         if left >= len(self.tree):\n",
    "#             return idx\n",
    "\n",
    "#         if s <= self.tree[left]:\n",
    "#             return self._retrieve(left, s)\n",
    "#         else:\n",
    "#             return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "#     def total(self):\n",
    "#         return self.tree[0]\n",
    "\n",
    "#     # store priority and sample\n",
    "#     def add(self, p, data):\n",
    "#         idx = self.write + self.capacity - 1\n",
    "\n",
    "#         self.data[self.write] = data\n",
    "#         self.update(idx, p)\n",
    "\n",
    "#         self.write += 1\n",
    "#         if self.write >= self.capacity:\n",
    "#             self.write = 0\n",
    "\n",
    "#         if self.n_entries < self.capacity:\n",
    "#             self.n_entries += 1\n",
    "\n",
    "#     # update priority\n",
    "#     def update(self, idx, p):\n",
    "#         change = p - self.tree[idx]\n",
    "\n",
    "#         self.tree[idx] = p\n",
    "#         self._propagate(idx, change)\n",
    "\n",
    "#     # get priority and sample\n",
    "#     def get(self, s):\n",
    "#         idx = self._retrieve(0, s)\n",
    "#         dataIdx = idx - self.capacity + 1\n",
    "\n",
    "#         return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "\n",
    "# class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "#     e = 0.01\n",
    "#     a = 0.6\n",
    "#     beta = 0.4\n",
    "#     absolute_error_upper = 1.\n",
    "#     beta_increment_per_sampling = 0.001\n",
    "\n",
    "#     def __init__(self, capacity):\n",
    "#         self.tree = SumTree(capacity)\n",
    "#         self.capacity = capacity\n",
    "\n",
    "#     def _get_priority(self, error):\n",
    "#         return (error + self.e) ** self.a\n",
    "        \n",
    "\n",
    "#     def add(self, sample):\n",
    "#         max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "#         if max_priority == 0:\n",
    "#             max_priority = self.absolute_error_upper\n",
    "#         self.tree.add(max_priority, sample)\n",
    "\n",
    "#     def sample(self, n):\n",
    "#         batch = []\n",
    "#         idxs = []\n",
    "#         segment = self.tree.total() / n\n",
    "#         priorities = []\n",
    "\n",
    "#         self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "#         for i in range(n):\n",
    "#             a = segment * i\n",
    "#             b = segment * (i + 1)\n",
    "\n",
    "#             s = random.uniform(a, b)\n",
    "#             (idx, p, data) = self.tree.get(s)\n",
    "#             priorities.append(p)\n",
    "#             batch.append(data)\n",
    "#             idxs.append(idx)\n",
    "\n",
    "#         sampling_probabilities = priorities / self.tree.total()\n",
    "#         is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "#         is_weight /= is_weight.max()\n",
    "\n",
    "#         return batch, idxs, is_weight\n",
    "\n",
    "#     def update(self, idx, error):\n",
    "#         p = self._get_priority(error)\n",
    "#         self.tree.update(idx, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, HIDDEN_LAYER)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.adv = nn.Linear(HIDDEN_LAYER, N_ACTIONS)\n",
    "        nn.init.xavier_uniform_(self.adv.weight) \n",
    "    \n",
    "        self.val = nn.Linear(HIDDEN_LAYER, 1)\n",
    "        nn.init.xavier_uniform_(self.val.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        adv = self.adv(x)\n",
    "        val = self.val(x)\n",
    "        \n",
    "        return val + adv - adv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3QN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "#         print(\"Neural net\")\n",
    "#         print(self.eval_net)\n",
    "\n",
    "        self.learn_step_counter  = 0 # for target updating\n",
    "#         self.memory_counter      = 0\n",
    "#         self.memory              = np.zeros((int(MEMORY_CAPACITY), N_STATES * 2 + 2)) # initialize memory\n",
    "        self.pmemory = Memory(MEMORY_CAPACITY)\n",
    "#         self.good_memory_counter = 0 # for storing non-terminal memories\n",
    "#         self.good_memory         = np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "#         self.bad_memory_counter  = 0 # for storing terminal memories\n",
    "#         self.bad_memory          = np.zeros((int(MEMORY_CAPACITY/2), N_STATES * 2 + 2)) # initialize memory\n",
    "        \n",
    "        self.optimizer           = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func           = nn.MSELoss()\n",
    "        \n",
    "        self.running_loss        = 0\n",
    "        self.loss_rec            = 0 \n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "    \n",
    "    def choose_greedy_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "        # input only one sample\n",
    "        actions_value = self.eval_net.forward(x)\n",
    "        action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "        action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        self.pmemory.store([s,a,r,s_])\n",
    "#         transition = np.hstack((s, [a, r], s_))\n",
    "#         self.pmemory.add(transition)\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        tree_idx, batch, ISWeights_mb = self.pmemory.sample(BATCH_SIZE)\n",
    "        states_mb = np.array([each[0][0] for each in batch], ndmin=N_STATES)\n",
    "        actions_mb = np.array([each[0][1] for each in batch])\n",
    "        rewards_mb = np.array([each[0][2] for each in batch]) \n",
    "        next_states_mb = np.array([each[0][3] for each in batch], ndmin=N_STATES)\n",
    "        \n",
    "        b_s  = torch.FloatTensor(states_mb)\n",
    "        b_a  = torch.LongTensor( actions_mb.astype(int))\n",
    "        b_r  = torch.FloatTensor(rewards_mb)\n",
    "        b_s_ = torch.FloatTensor(next_states_mb)\n",
    "        \n",
    "        \n",
    "#         mini_batch, idxs, is_weights = self.pmemory.sample(BATCH_SIZE)\n",
    "#         b_memory = np.array(mini_batch)\n",
    "#         print(b_memory.shape)\n",
    "#         b_s  = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "#         b_a  = torch.LongTensor( b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "#         b_r  = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "#         b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval   = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        a_eval   = self.eval_net(b_s).max(1)[1].view(BATCH_SIZE, 1) #best action according to eval_net\n",
    "        q_next   = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.gather(1, a_eval)   # shape (batch, 1)\n",
    "\n",
    "        errors = torch.abs(q_eval - q_target).data.numpy()\n",
    "        \n",
    "        memory.batch_update(tree_idx, errors)\n",
    "\n",
    "        # update priority\n",
    "#         for i in range(BATCH_SIZE):\n",
    "#             idx = idxs[i]\n",
    "#             self.pmemory.update(idx, errors[i])\n",
    "\n",
    "        loss = self.loss_func(q_eval, q_target)*torch.from_numpy(is_weights).float()\n",
    "#         temp = loss.detach()\n",
    "        \n",
    "#         self.loss_rec = np.append(self.loss_rec, temp.mean())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "#         loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([100235, 100712, 101711, 102361, 102509, 103470, 104303, 104609,\n",
       "        105483, 105652, 106549, 107055, 107490, 108161, 108797, 109360,\n",
       "        110250, 110972, 111500, 112411, 112931, 113181, 113763, 114691,\n",
       "        114933, 115703, 116251, 116975, 117881, 118119, 119134, 119832],\n",
       "       dtype=int32),\n",
       " [[[array([-0.02766037,  0.00889004,  0.04657389,  0.04099805]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.02748257,  0.20331426,  0.04739385, -0.23663426])]],\n",
       "  [[array([-0.00642754,  0.62715971,  0.00246602, -0.89284784]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.00611565,  0.82224812, -0.01539094, -1.18475457])]],\n",
       "  [[array([ 0.06102853,  0.93739929, -0.01911503, -1.44849281]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.07977651,  1.13275096, -0.04808488, -1.74708627])]],\n",
       "  [[array([ 0.05462264,  1.14638433, -0.10656801, -1.81714006]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.07755033,  1.34251806, -0.14291081, -2.14094337])]],\n",
       "  [[array([ 0.00431512,  0.41714686, -0.06181559, -0.65422442]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.01265806,  0.61307257, -0.07490008, -0.96571388])]],\n",
       "  [[array([ 0.11804697,  1.58180604, -0.11944578, -2.31599418]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.14968309,  1.77779756, -0.16576566, -2.64292037])]],\n",
       "  [[array([ 0.12400958,  1.7296195 , -0.19210788, -2.74908607]),\n",
       "    1,\n",
       "    -10,\n",
       "    array([ 0.15860197,  1.92549906, -0.24708961, -3.09363324])]],\n",
       "  [[array([ 0.08728831,  1.35993921, -0.14024154, -2.1393491 ]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.11448709,  1.5561415 , -0.18302853, -2.47185913])]],\n",
       "  [[array([ 0.01802009, -0.00285287,  0.03987427, -0.04618931]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.01796303,  0.1916753 ,  0.03895048, -0.32602969])]],\n",
       "  [[array([ 0.05962642,  1.38520984, -0.14763675, -2.1746127 ]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.08733062,  1.58142915, -0.191129  , -2.50898749])]],\n",
       "  [[array([-0.01960204,  0.78765788, -0.03100357, -1.17429083]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.00384888,  0.98316873, -0.05448939, -1.47652977])]],\n",
       "  [[array([-0.04280976,  0.39833524,  0.00445475, -0.56523781]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.03484306,  0.59339441, -0.00685   , -0.85651397])]],\n",
       "  [[array([ 0.04561916,  0.95284254, -0.0477563 , -1.50590613]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.06467601,  1.1485101 , -0.07787443, -1.81310786])]],\n",
       "  [[array([-0.04633863,  0.36213806, -0.04587691, -0.61975414]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.03909587,  0.55786972, -0.05827199, -0.9265258 ])]],\n",
       "  [[array([ 0.1360359 ,  1.76780713, -0.20608965, -2.77725099]),\n",
       "    1,\n",
       "    -10,\n",
       "    array([ 0.17139205,  1.96366176, -0.26163467, -3.12497844])]],\n",
       "  [[array([-0.01084422,  0.60749735, -0.0206787 , -0.90059487]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.00130573,  0.80289331, -0.0386906 , -1.19970525])]],\n",
       "  [[array([-0.02160573,  0.15579247, -0.03277448, -0.33613003]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.01848988,  0.35136514, -0.03949708, -0.63896547])]],\n",
       "  [[array([ 0.00225647,  0.81797524, -0.07988979, -1.21572246]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.01861597,  1.01403162, -0.10420424, -1.53233168])]],\n",
       "  [[array([ 0.00446683,  0.81750088, -0.06345367, -1.22133022]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.02081685,  1.0133805 , -0.08788027, -1.53320119])]],\n",
       "  [[array([ 0.03199426,  0.14781737,  0.04031797, -0.30199724]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.03495061,  0.34234219,  0.03427802, -0.58169708])]],\n",
       "  [[array([-0.0382088 ,  0.37374033, -0.03183601, -0.6481291 ]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.030734  ,  0.569291  , -0.04479859, -0.95066468])]],\n",
       "  [[array([ 0.06509953,  0.7409703 , -0.09039976, -1.19616279]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.07991893,  0.93713867, -0.11432301, -1.51575517])]],\n",
       "  [[array([-0.03586745,  0.20924388, -0.01096053, -0.27353074]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.03168258,  0.40452049, -0.01643114, -0.5696504 ])]],\n",
       "  [[array([ 0.05862952,  0.8154908 , -0.01128579, -1.11723354]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.07493933,  1.01075903, -0.03363046, -1.41343519])]],\n",
       "  [[array([ 0.00196383,  0.95074968, -0.06731094, -1.48263976]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.02097882,  1.14662497, -0.09696373, -1.79556183])]],\n",
       "  [[array([ 0.06223626,  0.96677121, -0.08960457, -1.48649376]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.08157168,  1.16286356, -0.11933444, -1.80576076])]],\n",
       "  [[array([ 0.11057473,  1.35647216, -0.13261443, -2.13683701]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.13770418,  1.5526338 , -0.17535117, -2.46737036])]],\n",
       "  [[array([ 0.03709298,  1.17640573, -0.09455339, -1.81869951]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.06062109,  1.37244312, -0.13092738, -2.13919939])]],\n",
       "  [[array([ 0.03010364,  1.33527585, -0.07838864, -2.05694825]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.05680916,  1.5311063 , -0.1195276 , -2.37281454])]],\n",
       "  [[array([ 0.03422675,  0.38079611,  0.00266145, -0.60672943]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.04184267,  0.57588075, -0.00947314, -0.89857288])]],\n",
       "  [[array([ 0.05132757,  0.53760966,  0.03166929, -0.81411499]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([ 0.06207976,  0.73228392,  0.01538699, -1.09667074])]],\n",
       "  [[array([-0.03196712,  0.21475309, -0.02797587, -0.28340671]),\n",
       "    1,\n",
       "    1.0,\n",
       "    array([-0.02767206,  0.41026267, -0.033644  , -0.58478015])]]],\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.pmemory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7a1e081e78d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "tree_idx, batch, ISWeights_mb = self.pmemory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = D3QN()\n",
    "NO_OF_EPISODES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 4: Index tensor must have same dimensions as input tensor at /pytorch/aten/src/TH/generic/THTensorMath.c:581",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ac66971fee28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlearn_start_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning starts from EPISODE: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a2642b55fb7e>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# q_eval w.r.t the action in experience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mq_eval\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_a\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape (batch, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0ma_eval\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#best action according to eval_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mq_next\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_s_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# detach from graph, don't backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 4: Index tensor must have same dimensions as input tensor at /pytorch/aten/src/TH/generic/THTensorMath.c:581"
     ]
    }
   ],
   "source": [
    "print('\\nCollecting experience...')\n",
    "TIMESTEP_LIMIT = 200\n",
    "total_time_steps = 0\n",
    "time_rec = []\n",
    "learn_start_flag = True\n",
    "learn_start = 0\n",
    "solve_metric = 195\n",
    "upgrade_flag = False\n",
    "upgrade_counter = 0\n",
    "state_max  = np.ones(N_STATES)*(-100)\n",
    "state_min  = np.ones(N_STATES)*(100)\n",
    "for i_episode in range(NO_OF_EPISODES):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    time_steps = 0\n",
    "    while True:\n",
    "#         env.render()\n",
    "        state_max = np.maximum(state_max,s) \n",
    "        state_min = np.minimum(state_min,s)\n",
    "        time_steps += 1\n",
    "        total_time_steps += 1\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "        if done:\n",
    "            if time_steps >= TIMESTEP_LIMIT:\n",
    "                r = 10\n",
    "            else:\n",
    "                r = -10\n",
    "           \n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "        \n",
    "        if i_episode >= 2000:\n",
    "            dqn.learn()\n",
    "            if learn_start_flag:\n",
    "                print(\"Learning starts from EPISODE: \",i_episode)\n",
    "                learn_start = i_episode\n",
    "                learn_start_flag = False\n",
    "        \n",
    "        if done:\n",
    "            time_rec = np.append(time_rec, time_steps)\n",
    "            break\n",
    "        s = s_\n",
    "    \n",
    "    #if minimum of episode length of last 100 episodes is greater than upgrade_metric=195\n",
    "    if time_rec[-100:].min() > solve_metric:\n",
    "        break\n",
    "#         upgrade_counter += 1\n",
    "#     else:\n",
    "#         upgrade_counter = 0\n",
    "        \n",
    "#     if upgrade_counter > 110:\n",
    "#         upgrade_counter = 0\n",
    "#         print(\"Upgrading @ EPISODE: \", i_episode)\n",
    "# #         upgrade_metric *= 2\n",
    "# #         LR *= 0.1\n",
    "#         EPSILON += 0.02\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 10\n",
    "lossavg = running_mean(dqn.loss_rec, WINDOW)\n",
    "plt.plot(dqn.loss_rec,alpha=0.5,color='g')\n",
    "plt.plot(lossavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 100\n",
    "ravg = running_mean(time_rec, WINDOW)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15,6))\n",
    "\n",
    "ax2 = fig.add_subplot(2, 1, 1)\n",
    "data = time_rec\n",
    "ax2.plot(data, color = 'g',alpha=0.5)\n",
    "ax2.plot(np.ones_like(data)*1500, 'g--')\n",
    "ax2.plot(np.ones_like(data)*200, 'r--')\n",
    "\n",
    "ax2.set_xlabel('Iterations',color = 'k')\n",
    "ax2.set_ylabel('Time Steps',color = 'g')\n",
    "ax2.set_ylim([1,2.5e2])\n",
    "fig.tight_layout()\n",
    "ax2.grid()\n",
    "\n",
    "\n",
    "plt.plot(ravg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = './models/BASE_F' + datetime.now().strftime(\"_%H_%M_%S\")\n",
    "print(MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn.eval_net.state_dict(), MODELNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOTAL TIMESTEPS: \", total_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_time_rec = []\n",
    "for i_episode in range(1000):\n",
    "    env.length   = 0.5 + np.random.uniform(-0.3,0.3)\n",
    "#     xtra = [env.length]\n",
    "    s = env.reset()\n",
    "#     s = np.append(s, xtra)\n",
    "    time_steps = 0\n",
    "    while True:\n",
    "#         env.render()\n",
    "        time_steps += 1\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        # take action\n",
    "        s_, r, done, info = env.step(a)\n",
    "#         s_ = np.append(s_, xtra)\n",
    "\n",
    "        if done or time_steps >= TIMESTEP_LIMIT:\n",
    "            ttest_time_rec = np.append(ttest_time_rec, time_steps)\n",
    "            break\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,6))\n",
    "\n",
    "ax2 = fig.add_subplot(2, 1, 1)\n",
    "data = ttest_time_rec\n",
    "ax2.plot(data, color = 'g')\n",
    "ax2.plot(np.ones_like(data)*1500, 'g--')\n",
    "ax2.plot(np.ones_like(data)*200, 'r--')\n",
    "\n",
    "ax2.set_xlabel('Iterations',color = 'k')\n",
    "ax2.set_ylabel('Time Steps',color = 'g')\n",
    "ax2.set_ylim([1,2.5e2])\n",
    "fig.tight_layout()\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
